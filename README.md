# Frame Sequence Dataset - README

"Shajimon recives cheese attack! It’s super effective!."
                                            —Dex, Poke
## Overview
## ._.

This dataset class loads sequences of 4 consecutive frames along with their corresponding label for training models on temporal/sequential image data.



> **Research software** — Embedded Systems & Instrumentation Department, ESIGELEC, France.  
> Redistribution is forbidden under any circumstances.
> Copyright © 2026 OQUETUQUETAMOLECORPORATIONS



---

## Table of Contents


- [Overview](#overview)
- [Architecture](#architecture)
- [Dataset](#dataset)
  - [Generating Synthetic Data (MATLAB)](#generating-synthetic-data-matlab)
  - [Directory Structure](#directory-structure)
  - [File Naming Convention](#file-naming-convention)
- [Installation](#installation)
- [Usage](#usage)
  - [Training](#training)
  - [Testing](#testing)
- [Configuration](#configuration)
- [Known Issues & Code Notes](#known-issues--code-notes)

---

## Overview

The task is to segment a specific square object from a scene where it may be partially occluded by circular or rectangular masks. Each sample consists of **4 frames** showing the scene from different viewpoints or time steps, each with different occlusion patterns. The model must fuse temporal information to produce a single segmentation label.

| Input | Output |
|-------|--------|
| 4 × grayscale frames `(1, 4, H, W)` | Binary mask `(1, H, W)` |

![Sample sequence showing 4 frames and the corresponding label](sample_sequence.png)

---

## Architecture

SQUNET is UNNET encoder–decoder based on the U-Net architecture from https://arxiv.org/abs/2102.05229:


- **Encoder**: 7 stacked `ENCODERBLOCK` modules. Each block applies 3D convolutions across the temporal dimension, followed by a `FeatureFusionOperation` that collapses the frame axis into a 2D skip connection via a `Conv3d(kernel=(T,1,1))` and bilinear interpolation.
- **Decoder**: 6 stacked `DECODERBLOCK` modules. Each block upsamples, then fuses the 2D skip connection using a `ChannelAttentionBlock` (global average pooling → channel-wise sigmoid gating), followed by a residual 2D conv block.
- **Output**: A `Conv2DOutput` layer produces a single-channel sigmoid map.

```
Input (B, 1, 4, 512, 512)
     │
  ENCODER ×7   [3D convolutions, temporal fusion → 2D skip connections]
     │
  BOTTLENECK
     │
  DECODER ×6   [2D upsample + channel attention skip fusion]
     │
Output (B, 1, 512, 512)
```

The loss function is the **Dice loss**:

```
L = 1 − (2 · Σ(pred · target) + 1) / (Σ(pred + target) + 1)
```

---

## Dataset

### Generating Synthetic Data (MATLAB)

The synthetic dataset is generated by `SquareFrame.m`. Each sample contains:

- A **target square** (the object to segment), with a random occlusion (circle or rectangle) covering part of it — position, size, and color vary per frame.
- 4 additional **distractor squares**, each with their own random occlusion.
- Up to 3 **distractor circles** placed randomly.

The MATLAB script validates each sample by checking that the grayscale image has exactly `maxNumCircles + 2` connected components (4-connectivity). Samples that fail this check are discarded and regenerated.

**Parameters to configure in `HybridUnetDataset.m`:**

| Variable | Default | Description |
|---|---|---|
| `d` | `512` | Image size (pixels, square) |
| `numImages` | `20` | Number of valid samples to generate |
| `numSquares` | `5` | Total squares per frame (1 target + 4 distractors) |
| `maxNumCircles` | `3` | Number of distractor circles |
| `file_path` | `./dataset/test` | Output directory (`train` or `test`) |

Run the script twice — once with `file_path = "./dataset/train"` and once with `./dataset/test` — to produce both splits.

### Directory Structure

```
dataset/
├── train/
│   ├── images/
│   │   ├── image_001_frame_001.png
│   │   ├── image_001_frame_002.png
│   │   ├── image_001_frame_003.png
│   │   ├── image_001_frame_004.png
│   │   ├── image_002_frame_001.png
│   │   └── ...
│   └── labels/
│       ├── label_001.png
│       ├── label_002.png
│       └── ...
└── test/
    ├── images/
    └── labels/
```

### File Naming Convention

Images: `image_<SAMPLE_ID>_frame_<FRAME_NUM>.png`  
Labels: `label_<SAMPLE_ID>.png`

- `SAMPLE_ID` — zero-padded 3-digit integer (e.g. `001`, `042`)
- `FRAME_NUM` — 1-indexed frame number (`001`–`004`)
- Each sample has exactly 4 frames and 1 label

The `FramePictures` dataset class parses these names automatically. A **sliding window** is used so that if a sample has more than 4 frames, multiple overlapping sequences of 4 are created.

---

## Installation

```bash
pip install torch torchvision opencv-python numpy matplotlib
```

MATLAB R2021a or later is required to run `SquareFrame.m` (uses `insertShape` from the Computer Vision Toolbox).

---

## Usage

### Training

```bash
python main.py \
  -job TRAIN \
  -data_path ./dataset/train \
  -test_path  ./dataset/test \
  -model_file ./model/model.pth \
  -log_file   ./logs/training.csv \
  -num_epoch  200
```

To resume from a checkpoint:

```bash
python main.py \
  -job TRAIN \
  -data_path  ./dataset/train \
  -test_path  ./dataset/test \
  -model_file ./model/model.pth \
  -log_file   ./logs/training.csv \
  -num_epoch  400 \
  -state_file ./model/model_00100.pth
```

### Testing

```bash
python main.py \
  -job TEST \
  -test_path    ./dataset/test \
  -model_file   ./model/model.pth \
  -results_path ./results
```

Results are saved as PNG files in `results_path`, each showing the 4 input frames, the ground-truth label, and the model prediction side by side. A `loss.csv` summary is also written.

### All CLI Arguments

| Argument | Required for | Description |
|---|---|---|
| `-job` | Both | `TRAIN` or `TEST` |
| `-data_path` | Train | Training dataset directory |
| `-test_path` | Both | Test/validation dataset directory |
| `-model_file` | Both | Path to save/load model model (`.pth`) |
| `-log_file` | Train | Path to CSV training log |
| `-num_epoch` | Train | Total number of epochs |
| `-state_file` | Train (optional) | Checkpoint to resume from |
| `-results_path` | Test | Directory for output prediction images |
---

## Configuration

The following hyperparameters are set at the top of `main.py`:

| Variable | Default | Description |
|---|---|---|
| `num_batch` | `10` | Batch size |
| `num_workers` | `4` | DataLoader worker threads |
| `learning_rate` | `1e-3` | Initial learning rate |
| `min_learning_rate` | `1e-5` | Floor for LR scheduler |
| `nfilter` | `64` | Base filter count (model is instantiated with `nfilter=8`) |
| `augment_noise` | `0.025` | Gaussian noise σ ceiling for training augmentation |
| `save_frequency` | `10` | Save a checkpoint every N epochs |
| `load_first` | `False` | Preload all data into RAM |

> **Note:** `nfilter=64` in the config comment but the model is created with `nfilter=8`. With `nfilter=8` the encoder channels are `8→16→32→64→128→256→512`. Change this value in the `SVSUNET(in_c=1, nfilter=8)` calls inside `train()` and `test()` if you want a larger model.

**Data augmentation** (applied only during training when `noise` is set):
- **Horizontal flip** with 50% probability — applied identically to all frames in a sequence and to the label.
- **Additive Gaussian noise** with σ ~ Uniform(0, `augment_noise`) — applied to frames only.

---

## Known Issues & Code Notes

If I tell you It is no fun 

---

 ## License
Copyright © 2026 OQUETUQUETAMOLECORPORATIONS
